{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    [
     "# Phase 2: Supervised Pattern Recognition\n",
     "\n",
     "This notebook implements supervised learning models for AMR pattern prediction.\n",
     "\n",
     "## Objectives\n",
     "- Train 6 classifiers: Random Forest, XGBoost, Logistic Regression, SVM, KNN, Naive Bayes\n",
     "- Perform two classification tasks:\n",
     "  1. **MAR Classification**: Binary (High MAR vs Low MAR)\n",
     "  2. **Species Classification**: Multi-class (bacterial species)\n",
     "- Evaluate models and select the best performers\n",
     "- Analyze feature importance"
    ]
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    [
     "## 1. Setup and Imports"
    ]
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    [
     "# Standard libraries\n",
     "import pandas as pd\n",
     "import numpy as np\n",
     "import matplotlib.pyplot as plt\n",
     "import seaborn as sns\n",
     "import joblib\n",
     "import json\n",
     "import os\n",
     "\n",
     "# Ignore warnings\n",
     "import warnings\n",
     "warnings.filterwarnings('ignore')\n",
     "\n",
     "# Custom modules\n",
     "from src.models.supervised import (\n",
     "    get_classifier,\n",
     "    train_classifier,\n",
     "    get_all_classifiers,\n",
     "    get_param_grid,\n",
     "    tune_hyperparameters,\n",
     "    predict,\n",
     "    predict_proba\n",
     ")\n",
     "from src.models.evaluation import (\n",
     "    evaluate_classifier,\n",
     "    get_confusion_matrix,\n",
     "    get_classification_report,\n",
     "    calculate_roc_auc,\n",
     "    get_feature_importance,\n",
     "    compare_models,\n",
     "    get_best_model\n",
     ")\n",
     "from src.visualization.plots import (\n",
     "    plot_confusion_matrix,\n",
     "    plot_roc_curve,\n",
     "    plot_roc_curves_comparison,\n",
     "    plot_model_comparison,\n",
     "    plot_metrics_heatmap,\n",
     "    plot_feature_importance,\n",
     "    plot_feature_importance_comparison,\n",
     "    plot_multiclass_confusion_matrix,\n",
     "    plot_per_class_metrics\n",
     ")\n",
     "\n",
     "# Set random seed\n",
     "np.random.seed(42)\n",
     "\n",
     "# Set plot style\n",
     "plt.style.use('seaborn-v0_8-darkgrid')\n",
     "sns.set_palette('husl')\n",
     "\n",
     "print('All imports successful!')"
    ]
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    [
     "## 2. Load Processed Data"
    ]
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    [
     "# Load train/validation/test splits\n",
     "X_train = pd.read_csv('../data/processed/X_train.csv')\n",
     "X_val = pd.read_csv('../data/processed/X_val.csv')\n",
     "X_test = pd.read_csv('../data/processed/X_test.csv')\n",
     "\n",
     "# Load MAR targets\n",
     "y_train_mar = pd.read_csv('../data/processed/y_train_mar.csv').squeeze()\n",
     "y_val_mar = pd.read_csv('../data/processed/y_val_mar.csv').squeeze()\n",
     "y_test_mar = pd.read_csv('../data/processed/y_test_mar.csv').squeeze()\n",
     "\n",
     "# Load Species targets\n",
     "y_train_species = pd.read_csv('../data/processed/y_train_species.csv').squeeze()\n",
     "y_val_species = pd.read_csv('../data/processed/y_val_species.csv').squeeze()\n",
     "y_test_species = pd.read_csv('../data/processed/y_test_species.csv').squeeze()\n",
     "\n",
     "# Load feature names\n",
     "with open('../data/processed/feature_names.json', 'r') as f:\n",
     "    feature_data = json.load(f)\n",
     "    feature_names = feature_data['encoded_features']\n",
     "\n",
     "print(f'Training samples: {len(X_train)}')\n",
     "print(f'Validation samples: {len(X_val)}')\n",
     "print(f'Test samples: {len(X_test)}')\n",
     "print(f'Number of features: {X_train.shape[1]}')"
    ]
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    [
     "### 2.1 Display Class Distributions"
    ]
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    [
     "# MAR class distribution\n",
     "print('MAR Class Distribution (Train):')\n",
     "print(y_train_mar.value_counts())\n",
     "print(f'\\nMAR Class Proportions (Train):')\n",
     "print(y_train_mar.value_counts(normalize=True))\n",
     "\n",
     "print('\\n' + '='*50)\n",
     "print('\\nSpecies Class Distribution (Train):')\n",
     "print(y_train_species.value_counts())\n",
     "print(f'\\nSpecies Class Proportions (Train):')\n",
     "print(y_train_species.value_counts(normalize=True))"
    ]
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    [
     "## 3. MAR Classification (Binary: High MAR vs Low MAR)\n",
     "\n",
     "Train all 6 classifiers to predict whether an isolate has high or low MAR index."
    ]
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    [
     "### 3.1 Train All Classifiers"
    ]
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    [
     "# Get all classifiers\n",
     "classifiers_mar = get_all_classifiers(random_state=42)\n",
     "\n",
     "# Dictionary to store results\n",
     "mar_results = {}\n",
     "mar_models = {}\n",
     "mar_predictions = {}\n",
     "\n",
     "print('Training MAR classifiers...\\n')\n",
     "\n",
     "for name, clf in classifiers_mar.items():\n",
     "    print(f'Training {name}...')\n",
     "    \n",
     "    # Train the model\n",
     "    trained_clf = train_classifier(clf, X_train, y_train_mar)\n",
     "    \n",
     "    # Make predictions on validation set\n",
     "    y_val_pred = predict(trained_clf, X_val)\n",
     "    y_val_proba = predict_proba(trained_clf, X_val)\n",
     "    \n",
     "    # Get probabilities for positive class (if available)\n",
     "    if y_val_proba is not None and y_val_proba.ndim > 1:\n",
     "        y_val_proba_pos = y_val_proba[:, 1]\n",
     "    else:\n",
     "        y_val_proba_pos = y_val_proba\n",
     "    \n",
     "    # Evaluate\n",
     "    metrics = evaluate_classifier(y_val_mar, y_val_pred, y_val_proba_pos)\n",
     "    \n",
     "    # Store results\n",
     "    mar_results[name] = metrics\n",
     "    mar_models[name] = trained_clf\n",
     "    mar_predictions[name] = {\n",
     "        'y_pred': y_val_pred,\n",
     "        'y_proba': y_val_proba_pos\n",
     "    }\n",
     "    \n",
     "    print(f'  Validation F1: {metrics[\"f1\"]:.4f}')\n",
     "    print(f'  Validation AUC: {metrics.get(\"auc\", 0):.4f}\\n')\n",
     "\n",
     "print('Training complete!')"
    ]
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    [
     "### 3.2 Validation Results Comparison"
    ]
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    [
     "# Create comparison DataFrame\n",
     "mar_comparison = compare_models(mar_results)\n",
     "print('MAR Classification - Validation Results:\\n')\n",
     "print(mar_comparison.round(4))"
    ]
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    [
     "# Visualize model comparison\n",
     "fig = plot_model_comparison(mar_comparison, metric='f1', \n",
     "                            title='MAR Classification: F1-Score Comparison')\n",
     "plt.tight_layout()\n",
     "plt.savefig('../reports/figures/mar_model_comparison_f1.png', dpi=300, bbox_inches='tight')\n",
     "plt.show()\n",
     "\n",
     "# Metrics heatmap\n",
     "fig = plot_metrics_heatmap(mar_comparison, title='MAR Classification: All Metrics')\n",
     "plt.tight_layout()\n",
     "plt.savefig('../reports/figures/mar_metrics_heatmap.png', dpi=300, bbox_inches='tight')\n",
     "plt.show()"
    ]
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    [
     "### 3.3 ROC Curves Comparison"
    ]
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    [
     "# Prepare data for ROC curves comparison\n",
     "roc_data = {}\n",
     "for name, preds in mar_predictions.items():\n",
     "    if preds['y_proba'] is not None:\n",
     "        roc_data[name] = {\n",
     "            'y_true': y_val_mar,\n",
     "            'y_proba': preds['y_proba']\n",
     "        }\n",
     "\n",
     "# Plot ROC curves\n",
     "if roc_data:\n",
     "    fig = plot_roc_curves_comparison(roc_data, title='MAR Classification: ROC Curves')\n",
     "    plt.tight_layout()\n",
     "    plt.savefig('../reports/figures/mar_roc_curves.png', dpi=300, bbox_inches='tight')\n",
     "    plt.show()"
    ]
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    [
     "### 3.4 Evaluate Best Models on Test Set"
    ]
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    [
     "# Get best model based on F1 score\n",
     "best_mar_model_name = get_best_model(mar_results, metric='f1')\n",
     "print(f'Best MAR model (by F1): {best_mar_model_name}')\n",
     "\n",
     "# Evaluate best model on test set\n",
     "best_mar_model = mar_models[best_mar_model_name]\n",
     "y_test_pred = predict(best_mar_model, X_test)\n",
     "y_test_proba = predict_proba(best_mar_model, X_test)\n",
     "\n",
     "if y_test_proba is not None and y_test_proba.ndim > 1:\n",
     "    y_test_proba_pos = y_test_proba[:, 1]\n",
     "else:\n",
     "    y_test_proba_pos = y_test_proba\n",
     "\n",
     "# Evaluate\n",
     "test_metrics = evaluate_classifier(y_test_mar, y_test_pred, y_test_proba_pos)\n",
     "\n",
     "print(f'\\nTest Set Performance:')\n",
     "for metric, value in test_metrics.items():\n",
     "    print(f'  {metric}: {value:.4f}')"
    ]
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    [
     "# Confusion matrix for best model\n",
     "cm = get_confusion_matrix(y_test_mar, y_test_pred)\n",
     "class_names = ['Low MAR', 'High MAR']\n",
     "\n",
     "fig = plot_confusion_matrix(cm, class_names, \n",
     "                            title=f'MAR Classification: {best_mar_model_name} (Test Set)')\n",
     "plt.tight_layout()\n",
     "plt.savefig(f'../reports/figures/mar_confusion_matrix_{best_mar_model_name}.png', \n",
     "            dpi=300, bbox_inches='tight')\n",
     "plt.show()"
    ]
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    [
     "### 3.5 Feature Importance Analysis"
    ]
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    [
     "# Get feature importance from tree-based models\n",
     "feature_importances = {}\n",
     "\n",
     "# Random Forest\n",
     "if 'random_forest' in mar_models:\n",
     "    rf_importance = get_feature_importance(mar_models['random_forest'], feature_names)\n",
     "    feature_importances['random_forest'] = rf_importance['importance'].values\n",
     "    \n",
     "    # Save to CSV\n",
     "    rf_importance.to_csv('../reports/results/feature_importances_rf_mar.csv', index=False)\n",
     "    \n",
     "    # Plot\n",
     "    fig = plot_feature_importance(rf_importance['importance'].values, \n",
     "                                  rf_importance['feature'].values,\n",
     "                                  top_n=15,\n",
     "                                  title='MAR: Random Forest Feature Importance')\n",
     "    plt.tight_layout()\n",
     "    plt.savefig('../reports/figures/mar_feature_importance_rf.png', dpi=300, bbox_inches='tight')\n",
     "    plt.show()\n",
     "\n",
     "# XGBoost\n",
     "if 'xgboost' in mar_models:\n",
     "    xgb_importance = get_feature_importance(mar_models['xgboost'], feature_names)\n",
     "    feature_importances['xgboost'] = xgb_importance['importance'].values\n",
     "    \n",
     "    # Save to CSV\n",
     "    xgb_importance.to_csv('../reports/results/feature_importances_xgb_mar.csv', index=False)\n",
     "    \n",
     "    # Plot\n",
     "    fig = plot_feature_importance(xgb_importance['importance'].values,\n",
     "                                  xgb_importance['feature'].values,\n",
     "                                  top_n=15,\n",
     "                                  title='MAR: XGBoost Feature Importance')\n",
     "    plt.tight_layout()\n",
     "    plt.savefig('../reports/figures/mar_feature_importance_xgb.png', dpi=300, bbox_inches='tight')\n",
     "    plt.show()"
    ]
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    [
     "# Compare feature importances between RF and XGBoost\n",
     "if len(feature_importances) >= 2:\n",
     "    fig = plot_feature_importance_comparison(feature_importances, feature_names, top_n=10,\n",
     "                                             title='MAR: Feature Importance Comparison')\n",
     "    plt.tight_layout()\n",
     "    plt.savefig('../reports/figures/mar_feature_importance_comparison.png', \n",
     "                dpi=300, bbox_inches='tight')\n",
     "    plt.show()"
    ]
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    [
     "## 4. Species Classification (Multi-class)\n",
     "\n",
     "Train all 6 classifiers to predict bacterial species from resistance profiles."
    ]
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    [
     "### 4.1 Train All Classifiers"
    ]
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    [
     "# Get all classifiers (with multi-class settings where needed)\n",
     "classifiers_species = get_all_classifiers(random_state=42)\n",
     "\n",
     "# Dictionary to store results\n",
     "species_results = {}\n",
     "species_models = {}\n",
     "species_predictions = {}\n",
     "\n",
     "print('Training Species classifiers...\\n')\n",
     "\n",
     "for name, clf in classifiers_species.items():\n",
     "    print(f'Training {name}...')\n",
     "    \n",
     "    # Train the model\n",
     "    trained_clf = train_classifier(clf, X_train, y_train_species)\n",
     "    \n",
     "    # Make predictions on validation set\n",
     "    y_val_pred = predict(trained_clf, X_val)\n",
     "    y_val_proba = predict_proba(trained_clf, X_val)\n",
     "    \n",
     "    # Evaluate\n",
     "    metrics = evaluate_classifier(y_val_species, y_val_pred, y_val_proba, average='weighted')\n",
     "    \n",
     "    # Store results\n",
     "    species_results[name] = metrics\n",
     "    species_models[name] = trained_clf\n",
     "    species_predictions[name] = {\n",
     "        'y_pred': y_val_pred,\n",
     "        'y_proba': y_val_proba\n",
     "    }\n",
     "    \n",
     "    print(f'  Validation F1 (weighted): {metrics[\"f1\"]:.4f}')\n",
     "    print(f'  Validation Accuracy: {metrics[\"accuracy\"]:.4f}\\n')\n",
     "\n",
     "print('Training complete!')"
    ]
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    [
     "### 4.2 Validation Results Comparison"
    ]
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    [
     "# Create comparison DataFrame\n",
     "species_comparison = compare_models(species_results)\n",
     "print('Species Classification - Validation Results:\\n')\n",
     "print(species_comparison.round(4))"
    ]
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    [
     "# Visualize model comparison\n",
     "fig = plot_model_comparison(species_comparison, metric='f1',\n",
     "                            title='Species Classification: F1-Score Comparison')\n",
     "plt.tight_layout()\n",
     "plt.savefig('../reports/figures/species_model_comparison_f1.png', dpi=300, bbox_inches='tight')\n",
     "plt.show()\n",
     "\n",
     "# Metrics heatmap\n",
     "fig = plot_metrics_heatmap(species_comparison, title='Species Classification: All Metrics')\n",
     "plt.tight_layout()\n",
     "plt.savefig('../reports/figures/species_metrics_heatmap.png', dpi=300, bbox_inches='tight')\n",
     "plt.show()"
    ]
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    [
     "### 4.3 Evaluate Best Model on Test Set"
    ]
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    [
     "# Get best model based on F1 score\n",
     "best_species_model_name = get_best_model(species_results, metric='f1')\n",
     "print(f'Best Species model (by F1): {best_species_model_name}')\n",
     "\n",
     "# Evaluate best model on test set\n",
     "best_species_model = species_models[best_species_model_name]\n",
     "y_test_pred = predict(best_species_model, X_test)\n",
     "y_test_proba = predict_proba(best_species_model, X_test)\n",
     "\n",
     "# Evaluate\n",
     "test_metrics = evaluate_classifier(y_test_species, y_test_pred, y_test_proba, average='weighted')\n",
     "\n",
     "print(f'\\nTest Set Performance:')\n",
     "for metric, value in test_metrics.items():\n",
     "    print(f'  {metric}: {value:.4f}')"
    ]
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    [
     "# Get classification report\n",
     "species_labels = sorted(y_train_species.unique())\n",
     "report = get_classification_report(y_test_species, y_test_pred, target_names=species_labels)\n",
     "\n",
     "print('\\nPer-Class Performance:\\n')\n",
     "report_df = pd.DataFrame(report).transpose()\n",
     "print(report_df.round(3))"
    ]
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    [
     "# Confusion matrix for best model\n",
     "cm = get_confusion_matrix(y_test_species, y_test_pred)\n",
     "species_labels = sorted(y_train_species.unique())\n",
     "\n",
     "fig = plot_multiclass_confusion_matrix(\n",
     "    cm, species_labels,\n",
     "    title=f'Species Classification: {best_species_model_name} (Test Set)'\n",
     ")\n",
     "plt.tight_layout()\n",
     "plt.savefig(f'../reports/figures/species_confusion_matrix_{best_species_model_name}.png',\n",
     "            dpi=300, bbox_inches='tight')\n",
     "plt.show()"
    ]
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    [
     "### 4.4 Per-Class Metrics Visualization"
    ]
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    [
     "# Plot per-class metrics\n",
     "fig = plot_per_class_metrics(report, title='Species Classification: Per-Class Metrics')\n",
     "plt.tight_layout()\n",
     "plt.savefig('../reports/figures/species_per_class_metrics.png', dpi=300, bbox_inches='tight')\n",
     "plt.show()"
    ]
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    [
     "## 5. Final Model Selection and Model Saving"
    ]
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    [
     "### 5.1 Best Models Summary"
    ]
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    [
     "print('='*60)\n",
     "print('FINAL MODEL SELECTION')\n",
     "print('='*60)\n",
     "\n",
     "print(f'\\nBest MAR Classification Model: {best_mar_model_name}')\n",
     "print('Test Performance:')\n",
     "for metric, value in test_metrics.items():\n",
     "    print(f'  {metric}: {value:.4f}')\n",
     "\n",
     "print(f'\\nBest Species Classification Model: {best_species_model_name}')\n",
     "# Re-evaluate to get fresh metrics\n",
     "y_test_pred_species = predict(best_species_model, X_test)\n",
     "y_test_proba_species = predict_proba(best_species_model, X_test)\n",
     "test_metrics_species = evaluate_classifier(y_test_species, y_test_pred_species, \n",
     "                                           y_test_proba_species, average='weighted')\n",
     "print('Test Performance:')\n",
     "for metric, value in test_metrics_species.items():\n",
     "    print(f'  {metric}: {value:.4f}')"
    ]
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    [
     "### 5.2 Save Results and Models"
    ]
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    [
     "# Save MAR classification results\n",
     "mar_comparison.to_csv('../reports/results/mar_classification_results.csv')\n",
     "print('Saved MAR classification results')\n",
     "\n",
     "# Save Species classification results\n",
     "species_comparison.to_csv('../reports/results/species_classification_results.csv')\n",
     "print('Saved Species classification results')\n",
     "\n",
     "# Save best models\n",
     "os.makedirs('../models', exist_ok=True)\n",
     "joblib.dump(best_mar_model, '../models/best_model_mar.pkl')\n",
     "print(f'Saved best MAR model: {best_mar_model_name}')\n",
     "\n",
     "joblib.dump(best_species_model, '../models/best_model_species.pkl')\n",
     "print(f'Saved best Species model: {best_species_model_name}')\n",
     "\n",
     "# Save confusion matrices\n",
     "mar_cms = {name: get_confusion_matrix(y_val_mar, preds['y_pred']) \n",
     "           for name, preds in mar_predictions.items()}\n",
     "joblib.dump(mar_cms, '../reports/results/mar_confusion_matrices.pkl')\n",
     "print('Saved MAR confusion matrices')\n",
     "\n",
     "species_cms = {name: get_confusion_matrix(y_val_species, preds['y_pred'])\n",
     "               for name, preds in species_predictions.items()}\n",
     "joblib.dump(species_cms, '../reports/results/species_confusion_matrices.pkl')\n",
     "print('Saved Species confusion matrices')\n",
     "\n",
     "print('\\nAll results saved successfully!')"
    ]
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    [
     "## 6. Summary\n",
     "\n",
     "### Key Findings\n",
     "\n",
     "1. **MAR Classification**:\n",
     "   - Best performing model identified\n",
     "   - Feature importance reveals which antibiotics best predict MDR status\n",
     "   \n",
     "2. **Species Classification**:\n",
     "   - Multi-class performance varies by species\n",
     "   - Some species are easier to classify than others\n",
     "   \n",
     "3. **Next Steps**:\n",
     "   - Proceed to notebook 04 for comprehensive model comparison\n",
     "   - Interpret biological significance of top features\n",
     "   - Consider ensemble methods if needed"
    ]
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}