{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 0: Data Understanding & Preparation\n",
    "\n",
    "This notebook demonstrates the complete data preprocessing pipeline for the AMR Pattern Recognition project.\n",
    "\n",
    "## Pipeline Overview\n",
    "1. Load and Explore Raw Data\n",
    "2. Data Cleaning\n",
    "3. Feature Encoding\n",
    "4. Target Variable Creation (MAR Index & Species)\n",
    "5. Data Splitting\n",
    "6. Save Processed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Custom modules\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.data.preprocessing import (\n",
    "    load_raw_data,\n",
    "    identify_antibiotic_columns,\n",
    "    clean_interpretation_values,\n",
    "    handle_missing_values,\n",
    "    encode_resistance,\n",
    "    standardize_species_labels,\n",
    "    calculate_mar_index,\n",
    "    create_mar_target,\n",
    "    prepare_species_target\n",
    ")\n",
    "from src.data.splitting import stratified_split\n",
    "from src.features.build_features import extract_resistance_features, create_feature_matrix\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "data_path = '../data/raw/rawdata.csv'\n",
    "df = load_raw_data(data_path)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of samples: {df.shape[0]}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column information\n",
    "print(\"Column names:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data types\n",
    "print(\"Data types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify antibiotic interpretation columns\n",
    "antibiotic_int_cols = identify_antibiotic_columns(df)\n",
    "print(f\"Number of antibiotic interpretation columns: {len(antibiotic_int_cols)}\")\n",
    "print(f\"\\nAntibiotic interpretation columns:\")\n",
    "for i, col in enumerate(antibiotic_int_cols, 1):\n",
    "    print(f\"{i}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for special characters and variations in interpretation values\n",
    "print(\"Unique interpretation values before cleaning:\")\n",
    "for col in antibiotic_int_cols[:5]:  # Show first 5 as example\n",
    "    unique_vals = df[col].unique()\n",
    "    print(f\"\\n{col}: {unique_vals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize interpretation values\n",
    "df = clean_interpretation_values(df, antibiotic_int_cols)\n",
    "\n",
    "print(\"\\nUnique interpretation values after cleaning:\")\n",
    "for col in antibiotic_int_cols[:5]:  # Show first 5 as example\n",
    "    unique_vals = df[col].unique()\n",
    "    print(f\"{col}: {unique_vals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values before handling\n",
    "print(\"Missing values in interpretation columns (before):\")\n",
    "missing_before = df[antibiotic_int_cols].isnull().sum()\n",
    "print(missing_before[missing_before > 0])\n",
    "print(f\"\\nTotal missing values: {df[antibiotic_int_cols].isnull().sum().sum()}\")\n",
    "print(f\"Rows with all antibiotics missing: {df[antibiotic_int_cols].isnull().all(axis=1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values - drop rows where ALL antibiotics are missing\n",
    "df = handle_missing_values(df, antibiotic_int_cols, strategy='drop')\n",
    "\n",
    "print(f\"\\nDataset shape after handling missing values: {df.shape}\")\n",
    "print(f\"Rows with all antibiotics missing (after): {df[antibiotic_int_cols].isnull().all(axis=1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize species names\n",
    "print(\"Species before standardization:\")\n",
    "print(df['bacterial_species'].value_counts())\n",
    "\n",
    "df = standardize_species_labels(df)\n",
    "\n",
    "print(\"\\nSpecies after standardization:\")\n",
    "print(df['bacterial_species'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode resistance interpretations: s=0, i=1, r=2\n",
    "df = encode_resistance(df, antibiotic_int_cols, method='ordinal')\n",
    "\n",
    "# Get encoded column names\n",
    "encoded_cols = [col for col in df.columns if col.endswith('_encoded')]\n",
    "print(f\"Number of encoded columns: {len(encoded_cols)}\")\n",
    "print(f\"\\nFirst 5 encoded columns:\")\n",
    "for col in encoded_cols[:5]:\n",
    "    print(f\"  {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show encoding distribution for sample columns\n",
    "print(\"Encoding distribution (s=0, i=1, r=2):\")\n",
    "for i, col in enumerate(encoded_cols[:3]):\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(df[col].value_counts(dropna=False).sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize encoding distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for i, col in enumerate(encoded_cols[:3]):\n",
    "    df[col].value_counts().sort_index().plot(kind='bar', ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution: {col}')\n",
    "    axes[i].set_xlabel('Encoded Value')\n",
    "    axes[i].set_ylabel('Count')\n",
    "    axes[i].set_xticklabels(['Susceptible (0)', 'Intermediate (1)', 'Resistant (2)'], rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Target Variable Creation\n",
    "\n",
    "### 4.1 MAR Index Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAR index\n",
    "df['MAR_index'] = calculate_mar_index(df, encoded_cols)\n",
    "\n",
    "print(\"MAR Index Statistics:\")\n",
    "print(df['MAR_index'].describe())\n",
    "print(f\"\\nMissing MAR values: {df['MAR_index'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MAR distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df['MAR_index'].dropna(), bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(0.2, color='red', linestyle='--', linewidth=2, label='Threshold (0.2)')\n",
    "axes[0].set_xlabel('MAR Index')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('MAR Index Distribution')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[1].boxplot(df['MAR_index'].dropna())\n",
    "axes[1].axhline(0.2, color='red', linestyle='--', linewidth=2, label='Threshold (0.2)')\n",
    "axes[1].set_ylabel('MAR Index')\n",
    "axes[1].set_title('MAR Index Box Plot')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary High_MAR target (threshold=0.2)\n",
    "df['High_MAR'] = create_mar_target(df, threshold=0.2)\n",
    "\n",
    "print(\"High_MAR class distribution:\")\n",
    "print(df['High_MAR'].value_counts())\n",
    "print(f\"\\nClass proportions:\")\n",
    "print(df['High_MAR'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize High_MAR distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "df['High_MAR'].value_counts().plot(kind='bar', color=['green', 'red'])\n",
    "plt.title('High MAR Class Distribution')\n",
    "plt.xlabel('High_MAR (0=Low, 1=High)')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Low MAR (0)', 'High MAR (1)'], rotation=0)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Species Classification Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show species value counts\n",
    "print(\"Species distribution (before merging rare species):\")\n",
    "species_counts = df['bacterial_species'].value_counts()\n",
    "print(species_counts)\n",
    "print(f\"\\nTotal unique species: {df['bacterial_species'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare species target (merge species with < 10 samples into 'Other')\n",
    "df = prepare_species_target(df, min_samples=10)\n",
    "\n",
    "print(\"Species distribution (after merging rare species):\")\n",
    "print(df['species_target'].value_counts())\n",
    "print(f\"\\nTotal unique species targets: {df['species_target'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize species distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "species_counts = df['species_target'].value_counts()\n",
    "species_counts.plot(kind='bar', color='steelblue')\n",
    "plt.title('Species Target Distribution (min_samples=10)')\n",
    "plt.xlabel('Species')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Splitting\n",
    "\n",
    "Split data into 70% train, 20% validation, 10% test with stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix\n",
    "X = create_feature_matrix(df)\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"\\nFeature names:\")\n",
    "print(X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare targets\n",
    "# For MAR target\n",
    "y_mar = df['High_MAR'].copy()\n",
    "\n",
    "# For species target\n",
    "y_species = df['species_target'].copy()\n",
    "\n",
    "print(f\"MAR target shape: {y_mar.shape}\")\n",
    "print(f\"Species target shape: {y_species.shape}\")\n",
    "print(f\"\\nFeature matrix has {X.isnull().sum().sum()} missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for MAR target with stratification\n",
    "X_train_mar, X_val_mar, X_test_mar, y_train_mar, y_val_mar, y_test_mar = stratified_split(\n",
    "    X, y_mar,\n",
    "    train_size=0.7,\n",
    "    val_size=0.2,\n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"MAR Target - Split sizes:\")\n",
    "print(f\"Training set: {X_train_mar.shape[0]} samples ({X_train_mar.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Validation set: {X_val_mar.shape[0]} samples ({X_val_mar.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test_mar.shape[0]} samples ({X_test_mar.shape[0]/len(X)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution in splits\n",
    "print(\"\\nMAR Target - Class distribution in splits:\")\n",
    "print(\"\\nTraining set:\")\n",
    "print(y_train_mar.value_counts())\n",
    "print(y_train_mar.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nValidation set:\")\n",
    "print(y_val_mar.value_counts())\n",
    "print(y_val_mar.value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nTest set:\")\n",
    "print(y_test_mar.value_counts())\n",
    "print(y_test_mar.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for Species target with stratification\n",
    "X_train_sp, X_val_sp, X_test_sp, y_train_sp, y_val_sp, y_test_sp = stratified_split(\n",
    "    X, y_species,\n",
    "    train_size=0.7,\n",
    "    val_size=0.2,\n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Species Target - Split sizes:\")\n",
    "print(f\"Training set: {X_train_sp.shape[0]} samples ({X_train_sp.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Validation set: {X_val_sp.shape[0]} samples ({X_val_sp.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test_sp.shape[0]} samples ({X_test_sp.shape[0]/len(X)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize split distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# MAR target distribution\n",
    "mar_data = pd.DataFrame({\n",
    "    'Train': y_train_mar.value_counts(),\n",
    "    'Validation': y_val_mar.value_counts(),\n",
    "    'Test': y_test_mar.value_counts()\n",
    "})\n",
    "mar_data.plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_title('MAR Target Distribution Across Splits')\n",
    "axes[0].set_xlabel('High_MAR Class')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticklabels(['Low (0)', 'High (1)'], rotation=0)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Species target distribution (top 5)\n",
    "sp_train_top = y_train_sp.value_counts().head(5)\n",
    "sp_val_top = y_val_sp.value_counts().head(5)\n",
    "sp_test_top = y_test_sp.value_counts().head(5)\n",
    "sp_data = pd.DataFrame({\n",
    "    'Train': sp_train_top,\n",
    "    'Validation': sp_val_top,\n",
    "    'Test': sp_test_top\n",
    "})\n",
    "sp_data.plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_title('Species Target Distribution (Top 5)')\n",
    "axes[1].set_xlabel('Species')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_xticklabels(sp_data.index, rotation=45, ha='right')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create processed data directory if it doesn't exist\n",
    "processed_dir = Path('../data/processed')\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Saving processed data to {processed_dir}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataset\n",
    "df.to_csv(processed_dir / 'cleaned_data.csv', index=False)\n",
    "print(\"✓ Saved cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature matrices (MAR target splits)\n",
    "X_train_mar.to_csv(processed_dir / 'X_train.csv', index=False)\n",
    "X_val_mar.to_csv(processed_dir / 'X_val.csv', index=False)\n",
    "X_test_mar.to_csv(processed_dir / 'X_test.csv', index=False)\n",
    "print(\"✓ Saved feature matrices (X_train, X_val, X_test)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save MAR targets\n",
    "y_train_mar.to_csv(processed_dir / 'y_train_mar.csv', index=False, header=['High_MAR'])\n",
    "y_val_mar.to_csv(processed_dir / 'y_val_mar.csv', index=False, header=['High_MAR'])\n",
    "y_test_mar.to_csv(processed_dir / 'y_test_mar.csv', index=False, header=['High_MAR'])\n",
    "print(\"✓ Saved MAR targets (y_train_mar, y_val_mar, y_test_mar)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Species targets\n",
    "y_train_sp.to_csv(processed_dir / 'y_train_species.csv', index=False, header=['species_target'])\n",
    "y_val_sp.to_csv(processed_dir / 'y_val_species.csv', index=False, header=['species_target'])\n",
    "y_test_sp.to_csv(processed_dir / 'y_test_species.csv', index=False, header=['species_target'])\n",
    "print(\"✓ Saved species targets (y_train_species, y_val_species, y_test_species)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature names\n",
    "feature_names = X.columns.tolist()\n",
    "with open(processed_dir / 'feature_names.json', 'w') as f:\n",
    "    json.dump(feature_names, f, indent=2)\n",
    "print(\"✓ Saved feature_names.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save encoding mappings\n",
    "encoding_info = {\n",
    "    'resistance_encoding': {\n",
    "        's': 0,\n",
    "        'i': 1,\n",
    "        'r': 2,\n",
    "        'description': 'Ordinal encoding for antibiotic resistance interpretations'\n",
    "    },\n",
    "    'mar_threshold': 0.2,\n",
    "    'species_min_samples': 10,\n",
    "    'num_features': len(feature_names),\n",
    "    'num_antibiotics': len(antibiotic_int_cols),\n",
    "    'train_size': 0.7,\n",
    "    'val_size': 0.2,\n",
    "    'test_size': 0.1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "with open(processed_dir / 'encoding_mappings.json', 'w') as f:\n",
    "    json.dump(encoding_info, f, indent=2)\n",
    "print(\"✓ Saved encoding_mappings.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary\n",
    "summary = f\"\"\"\n",
    "Data Preprocessing Summary\n",
    "==========================\n",
    "\n",
    "Original dataset:\n",
    "  - Samples: {df.shape[0]}\n",
    "  - Total columns: {df.shape[1]}\n",
    "  - Antibiotic columns: {len(antibiotic_int_cols)}\n",
    "\n",
    "Feature matrix:\n",
    "  - Features: {X.shape[1]}\n",
    "  - Samples: {X.shape[0]}\n",
    "\n",
    "MAR Target:\n",
    "  - High MAR samples: {(y_mar == 1).sum()} ({(y_mar == 1).sum()/len(y_mar)*100:.1f}%)\n",
    "  - Low MAR samples: {(y_mar == 0).sum()} ({(y_mar == 0).sum()/len(y_mar)*100:.1f}%)\n",
    "\n",
    "Species Target:\n",
    "  - Unique species: {y_species.nunique()}\n",
    "\n",
    "Data Splits:\n",
    "  - Training: {X_train_mar.shape[0]} samples (70%)\n",
    "  - Validation: {X_val_mar.shape[0]} samples (20%)\n",
    "  - Test: {X_test_mar.shape[0]} samples (10%)\n",
    "\n",
    "Saved Files:\n",
    "  - cleaned_data.csv\n",
    "  - X_train.csv, X_val.csv, X_test.csv\n",
    "  - y_train_mar.csv, y_val_mar.csv, y_test_mar.csv\n",
    "  - y_train_species.csv, y_val_species.csv, y_test_species.csv\n",
    "  - feature_names.json\n",
    "  - encoding_mappings.json\n",
    "\"\"\"\n",
    "\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATA PREPROCESSING COMPLETE!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nAll processed files saved to: {processed_dir.absolute()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
